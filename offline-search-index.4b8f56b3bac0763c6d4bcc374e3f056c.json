
































[{"body":"cOS-toolkit releases consist on container images that can be used to build derived against and the cos source tree itself.\ncOS supports different release channels, all the final and cache images used are tagged and pushed regularly to Quay Container Registry and can be pulled for inspection from the registry as well.\nThose are exactly the same images used during upgrades, and can also be used to build Linux derivatives from cOS.\nFor example, if you want to see locally what‚Äôs in a openSUSE cOS version , you can:\n$ docker run -ti --rm quay.io/costoolkit/releases-opensuse:cos-system-$VERSION /bin/bash Releases We are not releasing artifacts currently on tagging - the cOS CI generates ISO and images artifacts used for testing, so you can also try out cOS by downloading the artifact type you are interested into from the Github Actions page by picking the latest job. The artifacts are listed at the end of the workflow page.\n","categories":"","description":"How to get cOS\n","excerpt":"How to get cOS\n","ref":"/docs/getting-started/download/","tags":"","title":"Download"},{"body":"When building a cos-toolkit derivative, a common set of packages are provided already with a common default configuration. Some of the most notably are:\n systemd as init system grub for boot loader dracut for initramfs  Each cos-toolkit flavor (opensuse, ubuntu, fedora) ships their own set of base packages depending on the distribution they are based against. You can find the list of packages in the packages keyword in the corresponding values file for each flavor\n","categories":"","description":"Package stack for derivatives\n","excerpt":"Package stack for derivatives\n","ref":"/docs/creating-derivatives/package_stack/","tags":"","title":"Package stack"},{"body":"Cloud-init files in /system/oem, /oem and /usr/local/oem are applied in 5 different phases: boot, network, fs, initramfs and reconcile. All the available cloud-init keywords can be used in each stage. Additionally, it‚Äôs possible also to hook before or after a stage has run, each one has a specific stage which is possible to run steps: boot.after, network.before, fs.after etc.\nMultiple stages can be specified in a single cloud-init file.\nrootfs This is the earliest stage, running before switching root, just right after the root is mounted in /sysroot and before applying the immutable rootfs configuration. This stage is executed over initrd root, no chroot is applied.\nExample:\nname:\"Set persistent devices\"stage:rootfs:- name:\"Layout configuration\"environment_file:/run/cos/cos-layout.envenvironment:VOLUMES:\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"OVERLAY:\"tmpfs:25%\"initramfs This is still an early stage, running before switching root. Here you can apply radical changes to the booting setup of cOS. Despite this is executed before switching root this exection runs chrooted into the target root after the immutable rootfs is set up and ready.\nExample:\nname:\"Run something on initramfs\"stages:initramfs:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive touch /etc/something_important- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modeboot This stage is executed after initramfs has switched root, during the systemd bootup process.\nExample:\nname:\"Run something on boot\"stages:boot:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modefs This stage is executed when fs is mounted and is guaranteed to have access to COS_STATE and COS_PERSISTENT.\nExample:\nname:\"Run something on boot\"stages:fs:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |touch /usr/local/something- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modenetwork This stage is executed when network is available\nExample:\nname:\"Run something on boot\"stages:network:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Network is available, do something..reconcile This stage is executed 5m after boot and periodically each 60m.\nExample:\nname:\"Run something on boot\"stages:reconcile:- name:\"Setting\"if:'[ ! -f \"/run/sentinel\" ]'commands:- |touch /run/sentinel","categories":"","description":"Configure the system in the various stages: boot, initramfs, fs, network, reconcile\n","excerpt":"Configure the system in the various stages: boot, initramfs, fs, ‚Ä¶","ref":"/docs/customizing/stages/","tags":"","title":"Stages"},{"body":"You can either choose to build a cOS derivatives or run cOS as-is.\nDownload cOS You can just try out cOS from the vanilla images: they are systems with a minimal package set in order to boot. Download (if needed) the appriopriate artifact for your Booting medium. cOS can run in: VMs, baremetals and Cloud. The default login username/password is root/cos.\nInstall To install run cos-installer \u003cdevice\u003e to start the installation process. Remove the ISO/medium and reboot.\nNote: cos-installer supports other options as well. Run cos-installer --help to see a complete help.\nWhat to do next? Check out the customization section to customize cOS or the tutorial section for some already prepared recipe examples.\nBuild cOS derivatives The starting point to use cos-toolkit is to check out our examples folder in cos, see also creating bootable images or have a look on one of our sample repository.\nThe only requirement to build derivatives with cos-toolkit is docker installed, see Development notes for more details on how to build cos instead.\nWhat to do next? Check out how to create bootable images and how to create full blown derivatives\n","categories":"","description":"Getting started with cOS\n","excerpt":"Getting started with cOS\n","ref":"/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"ISO ISO images are shipping a cOS vanilla image and they have an installer to perform an automated installation. They can be used to burn USB sticks or CD/DVD used to boot baremetals.\nVirtual machines For booting into Virtual machines we offer QCOW2, OVA, and raw disk recovery images.\nQCOW2 QCOW2 images contains a pre-installed cOS vanilla system which can be booted via QEMU, e.g:\nqemu-system-x86_64 -m 2048 -hda \u003ccos-disk-image\u003e.raw -bios /usr/share/qemu/ovmf-x86_64.bin OVA OVA images contains a pre-installed cOS vanilla system that can be imported in Virtualbox, Vsphere and used also imported as AMI images.\nVagrant Download the vagrant box, and run:\nvagrant box add cos \u003ccos-box-image\u003e vagrant init cos vagrant up RAW disk images RAW disk images contains only the cOS recovery system. Those are typically used when creating derivatives images based on top of cOS.\nThey can be run with QEMU with:\nqemu-system-x86_64 -m 2048 -hda \u003ccos-disk-image\u003e.raw -bios /usr/share/qemu/ovmf-x86_64.bin Cloud Images At the moment we support Azure and AWS images among our artifacts. We publish also AWS images that can be re-used in packer templates for creating customized AMI images.\nImport an AWS image manually  Upload the raw image to an S3 bucket  aws s3 cp \u003ccos-raw-image\u003e s3://\u003cyour_s3_bucket\u003e Created the disk container JSON (container.json file) as:  { \"Description\": \"cOS Testing image in RAW format\", \"Format\": \"raw\", \"UserBucket\": { \"S3Bucket\": \"\u003cyour_s3_bucket\u003e\", \"S3Key\": \"\u003ccos-raw-image\u003e\" } } Import the disk as snapshot  aws ec2 import-snapshot --description \"cOS PoC\" --disk-container file://container.json  Followed the procedure described in AWS docs to register an AMI from snapshot. Used all default settings unless for the firmware, set to force to UEFI boot.\n  Launch instance with this simple userdata with at least a 16Gb boot disk:\n  name: \"Default deployment\" stages: rootfs.after: - name: \"Repart image\" layout: # It will partition a device including the given filesystem label or part label (filesystem label matches first) device: label: COS_RECOVERY # Only last partition can be expanded # expand_partition: # size: 4096 add_partitions: - fsLabel: COS_STATE size: 8192 pLabel: state - fsLabel: COS_PERSISTENT # unset size or 0 size means all available space # size: 0 # default filesystem is ext2 when omitted # filesystem: ext4 pLabel: persistent network: - if: '[ -f \"/run/cos/recovery_mode\" ]' name: \"Deploy cos-system\" commands: - | # Use `cos-deploy --docker-image \u003cimg-ref\u003e` to deploy a custom image # By default latest cOS gets deployed cos-deploy \u0026\u0026 shutdown -r +1 Importing a Google Cloud image manually  Upload the Google Cloud compressed disk to your bucket  gsutil cp \u003ccos-gce-image\u003e gs://\u003cyour_bucket\u003e/ Import the disk as an image  gcloud compute images create \u003cnew_image_name\u003e --source-uri=\u003cyour_bucket\u003e/\u003ccos-gce-image\u003e --guest-os-features=UEFI_COMPATIBLE Launch instance with this simple userdata with at least a 16Gb boot disk:  HINT: See here on how to add user-data to an instance\nname:\"Default deployment\"stages:rootfs.after:- name:\"Repart image\"layout:# It will partition a device including the given filesystem label or part label (filesystem label matches first)device:label:COS_RECOVERY# Only last partition can be expanded# expand_partition:# size: 4096add_partitions:- fsLabel:COS_STATEsize:8192pLabel:state- fsLabel:COS_PERSISTENT# unset size or 0 size means all available space# size: 0 # default filesystem is ext2 when omitted# filesystem: ext4pLabel:persistentnetwork:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Deploy cos-system\"commands:- |# Use `cos-deploy --docker-image \u003cimg-ref\u003e` to deploy a custom image # By default latest cOS gets deployed cos-deploy \u0026\u0026 shutdown -r +1Login By default you can login with the user root and password cos.\nSee the customization section for examples on how to persist username and password changes after installation.\n","categories":"","description":"Documents various methods for booting cOS vanilla images\n","excerpt":"Documents various methods for booting cOS vanilla images\n","ref":"/docs/getting-started/booting/","tags":"","title":"Booting"},{"body":"It‚Äôs possible to create standard container images which are consumable by the vanilla cOS images (ISO, Cloud Images, etc.) during the upgrade and deploy phase.\nThis allows anyone to create bootable images by just building and publishing a container image with the usual container workflow, and is a way to persist customizations. The image have to contain parts of the cos-toolkit in order to be bootable, an illustrative example can be:\nARG LUET_VERSION=0.16.7 FROM quay.io/luet/base:$LUET_VERSION AS luet FROM opensuse/leap:15.3 ARG ARCH=amd64 ENV ARCH=${ARCH} RUN zypper in -y ... RUN luet install -y \\ toolchain/yip \\ utils/installer \\ system/cos-setup \\ system/immutable-rootfs \\ system/grub-config \\ system/cloud-config \\ utils/k9s \\ utils/nerdctl # Other custom logic. E.g, customize statically the upgrade channel, default users, packages. ... The workflow would be then:\n docker build the image docker push the image to some registry cos-upgrade --docker-image --no-verify $IMAGE from a cOS machine  You can explore more examples in the example section on how to create bootable images\nNote : the image should provide at least grub, systemd and dracut, as are the common set of packages between derivatives. See also package stack\n","categories":"","description":"This document describes the requirements to create standard container images that can be used for `cOS` deployments\n","excerpt":"This document describes the requirements to create standard container ‚Ä¶","ref":"/docs/creating-derivatives/creating_bootable_images/","tags":"","title":"Creating bootable images"},{"body":"It is possible to install a custom cloud-init style file during install with --config to cos-installer or, it‚Äôs possible to add more files manually to the /oem folder after installation. The file will be placed under /usr/local/oem and will persist across reboots.\nBy default cOS and derivatives, are reading and executing cloud-init files in (lexicopgrahic) sequence present in /system/oem, /usr/local/cloud-config and /oem during boot. It is also possible to run cloud-init file in a different location from boot cmdline by using the cos.setup=.. option.\nWhile /system/oem is reserved for system configurations, for example to be included in the container image, the /oem folder instead is reserved for persistent cloud-init files to be executed in the various stages.\nFor example, if you want to change /etc/issue of the system persistently, you can create /usr/local/cloud-config/90_after_install.yaml with the following content:\n# The following is executed before fs is setted up:stages:fs:- name:\"After install\"files:- path:/etc/issuecontent:|Welcome, have fun!permissions:0644owner:0group:0systemctl:disable:- wicked- name:\"After install (second step)\"files:- path:/etc/motdcontent:|Welcome, have more fun!permissions:0644owner:0group:0For more examples you can find /system/oem inside cOS vanilla images containing files used to configure on boot a pristine cOS.\n","categories":"","description":"Persisting configurations in cOS and derivatives\n","excerpt":"Persisting configurations in cOS and derivatives\n","ref":"/docs/customizing/configuration_persistency/","tags":"","title":"Persistency"},{"body":"Below is a reference of all keys available in the cloud-init style files.\nstages:# \"network\" is the stage where network is expected to be up# It is called internally when network is available from # the cos-setup-network unit.network:# Here there are a list of # steps to be run in the network stage- name:\"Some setup happening\"files:- path:/tmp/foocontent:|testpermissions:0777owner:1000group:100commands:- echo \"test\"modules:- nvidiaenvironment:FOO:\"bar\"systctl:debug.exception-trace:\"0\"hostname:\"foo\"systemctl:enable:- foodisable:- barstart:- bazmask:- foobarauthorized_keys:user:- \"github:mudler\"- \"ssh-rsa ....\"dns:path:/etc/resolv.confnameservers:- 8.8.8.8ensure_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"delete_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"datasource:providers:- \"aws\"- \"digitalocean\"path:\"/etc/cloud-data\"The default cloud-config format is split into stages (initramfs, boot, network, initramfs, reconcile, called generically STAGE_ID below) see also stages that are emitted internally during the various phases by calling cos-setup STAGE_ID. steps (STEP_NAME below) defined for each stage are executed in order.\nEach cloud-config file is loaded and executed only at the apprioriate stage, this allows further components to emit their own stages at the desired time.\nThe cloud-init tool can be also run standalone, this helps debugging locally and also during development, you can find separate releases here, or just run it with docker:\ncat \u003c\u003cEOF | docker run -i --rm quay.io/costoolkit/releases-opensuse:cos-recovery-0.5.8 yip -s test - stages: test: - commands: - echo \"test\" EOF   Compatibility with Cloud Init format A subset of the official cloud-config spec is implemented.\nIf a yaml file starts with #cloud-config it is parsed as a standard cloud-init and automatically associated it to the boot stage. For example:\n#cloud-configusers:- name:\"bar\"passwd:\"foo\"groups:\"users\"ssh_authorized_keys:- faaapploossh_authorized_keys:- asddruncmd:- foohostname:\"bar\"write_files:- encoding:b64content:CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4path:/foo/barpermissions:\"0644\"owner:\"bar\"Is executed at boot, by using the standard cloud-config format.\nstages.STAGE_ID.STEP_NAME.name A description of the stage step. Used only when printing output to console.\nstages.STAGE_ID.STEP_NAME.files A list of files to write to disk.\nstages:default:- files:- path:/tmp/barcontent:|#!/bin/sh echo \"test\"permissions:0777owner:1000group:100stages.STAGE_ID.STEP_NAME.directories A list of directories to be created on disk. Runs before files.\nstages:default:- name:\"Setup folders\"directories:- path:\"/etc/foo\"permissions:0600owner:0group:0stages.STAGE_ID.STEP_NAME.dns A way to configure the /etc/resolv.conf file.\nstages:default:- name:\"Setup dns\"dns:nameservers:- 8.8.8.8- 1.1.1.1search:- foo.baroptions:- ..path:\"/etc/resolv.conf.bak\"stages.STAGE_ID.STEP_NAME.hostname A string representing the machine hostname. It sets it in the running system, updates /etc/hostname and adds the new hostname to /etc/hosts.\nstages:default:- name:\"Setup hostname\"hostname:\"foo\"stages.STAGE_ID.STEP_NAME.sysctl Kernel configuration. It sets /proc/sys/\u003ckey\u003e accordingly, similarly to sysctl.\nstages:default:- name:\"Setup exception trace\"systctl:debug.exception-trace:\"0\"stages.STAGE_ID.STEP_NAME.authorized_keys A list of SSH authorized keys that should be added for each user. SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME}, similarly for Gitlab with gitlab:${USERNAME}.\nstages:default:- name:\"Setup exception trace\"authorized_keys:mudler:- github:mudler- ssh-rsa:...stages.STAGE_ID.STEP_NAME.node If defined, the node hostname where this stage has to run, otherwise it skips the execution. The node can be also a regexp in the Golang format.\nstages:default:- name:\"Setup logging\"node:\"bastion\"stages.STAGE_ID.STEP_NAME.users A map of users and user info to set. Passwords can be also encrypted.\nThe users parameter adds or modifies the specified list of users. Each user is an object which consists of the following fields. Each field is optional and of type string unless otherwise noted. In case the user is already existing, the entry is ignored.\n name: Required. Login name of user gecos: GECOS comment of user passwd: Hash of the password to use for this user. Unencrypted strings are supported too. homedir: User‚Äôs home directory. Defaults to /home/name no-create-home: Boolean. Skip home directory creation. primary-group: Default group for the user. Defaults to a new group created named after the user. groups: Add user to these additional groups no-user-group: Boolean. Skip default group creation. ssh-authorized-keys: List of public SSH keys to authorize for this user system: Create the user as a system user. No home directory will be created. no-log-init: Boolean. Skip initialization of lastlog and faillog databases. shell: User‚Äôs login shell.  stages:default:- name:\"Setup users\"users:bastion:passwd:\"strongpassword\"homedir:\"/home/foostages.STAGE_ID.STEP_NAME.ensure_entities A user or a group in the entity format to be configured in the system\nstages:default:- name:\"Setup users\"ensure_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"stages.STAGE_ID.STEP_NAME.delete_entities A user or a group in the entity format to be pruned from the system\nstages:default:- name:\"Setup users\"delete_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"stages.STAGE_ID.STEP_NAME.modules A list of kernel modules to load.\nstages:default:- name:\"Setup users\"modules:- nvidiastages.STAGE_ID.STEP_NAME.systemctl A list of systemd services to enable, disable, mask or start.\nstages:default:- name:\"Setup users\"systemctl:enable:- systemd-timesyncd- croniemask:- purge-kernelsdisable:- crondstart:- croniestages.STAGE_ID.STEP_NAME.environment A map of variables to write in /etc/environment, or otherwise specified in environment_file\nstages:default:- name:\"Setup users\"environment:FOO:\"bar\"stages.STAGE_ID.STEP_NAME.environment_file A string to specify where to set the environment file\nstages:default:- name:\"Setup users\"environment_file:\"/home/user/.envrc\"environment:FOO:\"bar\"stages.STAGE_ID.STEP_NAME.timesyncd Sets the systemd-timesyncd daemon file (/etc/system/timesyncd.conf) file accordingly. The documentation for timesyncd and all the options can be found here.\nstages:default:- name:\"Setup NTP\"systemctl:enable:- systemd-timesyncdtimesyncd:NTP:\"0.pool.org foo.pool.org\"FallbackNTP:\"\"...stages.STAGE_ID.STEP_NAME.commands A list of arbitrary commands to run after file writes and directory creation.\nstages:default:- name:\"Setup something\"commands:- echo 1 \u003e /barstages.STAGE_ID.STEP_NAME.datasource Sets to fetch user data from the specified cloud providers. It populates provider specific data into /run/config folder and the custom user data is stored into the provided path.\nstages:default:- name:\"Fetch cloud provider's user data\"datasource:providers:- \"aws\"- \"digitalocean\"path:\"/etc/cloud-data\"","categories":"","description":"Features inherited by cOS derivatives that are also available in the cOS vanilla images\n","excerpt":"Features inherited by cOS derivatives that are also available in the ‚Ä¶","ref":"/docs/reference/cloud_init/","tags":"","title":"Cloud-init support"},{"body":"Runtime features There are present default cloud-init configurations files available under /system/features for example purposes, and to quickly enable testing features.\nFeatures are simply cloud-config yaml files in the above folder and can be enabled/disabled with cos-feature. For example, after install, to enable k3s it‚Äôs sufficient to type cos-feature enable k3s and reboot. Similarly, by adding a yaml file in the above folder will make it available for listing/enable/disable.\nSee cos-feature list for the available features.\n$\u003e cos-feature list ==================== cOS features list To enable, run: cos-feature enable \u003cfeature\u003e To disable, run: cos-feature disable \u003cfeature\u003e ==================== - carrier - harvester - k3s - vagrant (enabled) ... SELinux policy By default, derivatives have SELinux enabled in permissive mode. You can use the cos-toolkit default policy as a kickstart to customize on top.\nCopy the package (create a new folder with build.yaml, definition.yaml and cOS.te) into the derivative tree and customize to suit your needs, and add it as a build requirement to your OS package.\nNote: the cOS.te sample policy was created using the utility audit2allow after running some basic operations in permissive mode using system default policies. allow2audit translates audit messages into allow/dontaudit SELinux policies which can be later compiled as a SELinux module. This is the approach used in this illustration example and mostly follows audit2allow man pages.\n","categories":"","description":"Features inherited by cOS derivatives that are also available in the cOS vanilla images\n","excerpt":"Features inherited by cOS derivatives that are also available in the ‚Ä¶","ref":"/docs/reference/derivatives_featureset/","tags":"","title":"Derivatives featureset"},{"body":"This is a work in progress example of how to deploy K3S + Fleet + System Uprade Controller over a cOS vanilla image only by using cloud-init yaml configuration files. The config file reproduced here is meant to be included as a user-data in a cloud provider (aws, gcp, azure, etc) or as part of a cdrom (cOS-Recovery will try to fetch /userdata file from a cdrom device).\nA vanilla image is an image that only provides the cOS-Recovery system on a COS_RECOVERY partition. It does not include any other system and it is meant to be dumped to a bigger disk and deploy a cOS system or a derivative system over the free space in disk. COS vanilla images are build as part of the CI workflow, see CI artifacts to download one of those.\nThe configuration file of this example has two purposes: first it deploys cOS, second in reboots on the deployed OS and deploys K3S + Fleet + System Upgrades Controller.\nOn first boot it will fail to boot cOS grub menu entry and fallback to cOS-Recovery system. From there it will partition the vanilla image to create the main system partition (COS_STATE) and add an extra partition for persistent data (COS_PERSISTENT). It will use the full disk, a disk of at least 20GiB is recommended. After partitioning it will deploy the main system on COS_STATE and reboot to it.\nOn consequent boots it will simply boot from COS_STATE, there it prepares the persistent areas of the system (arranges few bind mounts inside COS_PERSISTENT) and then it runs an standard installation of K3s, Fleet and System Upgrade Controller. After few minutes after the system is up the K3s cluster is up and running.\nNote this setup similar to the derivative example using Fleet. The main difference is that this example does not require to build any image, it is pure cloud-init configuration based.\nUser data configuration file name:\"Default deployment\"stages:rootfs.after:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Repart image\"layout:# It will partition a device including the given filesystem label or part label (filesystem label matches first)device:label:COS_RECOVERYadd_partitions:- fsLabel:COS_STATE# 15Gb for COS_STATE, so the disk should have, at least, 20Gbsize:15360pLabel:state- fsLabel:COS_PERSISTENT# unset size or 0 size means all available spacepLabel:persistentinitramfs:- name:\"Set /etc/hosts\"files:- path:/etc/hostscontent:|127.0.0.1localhost- if:'[ ! -f \"/run/cos/recovery_mode\" ]'name:\"Persist\"commands:- |target=/usr/local/.cos-state # Always want the latest update of systemd conf from the image # TODO: This might break the fallback system mkdir -p \"${target}/etc/systemd/\" rsync -av /etc/systemd/ \"${target}/etc/systemd/\" # Only populate ssh conf once if [ ! -e \"${target}/etc/ssh\" ]; then mkdir -p \"${target}/etc/ssh/\" rsync -av /etc/ssh/ \"${target}/etc/ssh/\" fi # undo /home /opt /root mount from cos immutable-rootfs module # TODO: we could think of configuring custom overlay paths in # immutable rootfs package. So this part could be omitted for i in home opt root; do sed -i \"/overlay \\/${i} /d\" /etc/fstab nsenter -m -t 1 -- umount \"/sysroot/${i}\" done # setup directories as persistent # TODO: would it make sense defining persistent state overlayfs mounts # as part of the immutable rootfs config? for i in root opt home var/lib/rancher var/lib/kubelet etc/systemd etc/rancher etc/ssh; do mkdir -p \"${target}/${i}\" \"/${i}\" echo \"${target}/${i} /${i} none defaults,bind 0 0\" \u003e\u003e /etc/fstab nsenter -m -t 1 -- mount -o defaults,bind \"/sysroot${target}/${i}\" \"/sysroot/${i}\" done # ensure /var/log/journal exists so it's labeled correctly mkdir -p /var/log/journalnetwork.before:- name:\"Setup SSH keys\"authorized_keys:root:# It can download ssh key from remote places, such as github user keys (e.g. `github:my_user`)- my_custom_ssh_key- if:'[ ! -f \"/run/cos/recovery_mode\" ]'name:\"Fleet deployment\"files:- path:/etc/k3s/manifests/fleet-config.yamlcontent:|apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet-crd namespace: kube-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-crd-0.3.3.tgz --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet namespace: kube-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-0.3.3.tgznetwork:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Deploy cos-system\"commands:# Deploys the latest image available in default channel (quay.io/costoolkit/releases-opensuse)# use --docker-image to deploy a custom image# e.g. `cos-deploy --docker-image quay.io/my_custom_repo:my_image`- cos-deploy \u0026\u0026 shutdown -r now- if:'[ ! -f \"/run/cos/recovery_mode\" ]'name:\"Setup k3s\"directories:- path:\"/usr/local/bin\"permissions:0755owner:0group:0commands:- |curl -sfL https://get.k3s.io | \\ INSTALL_K3S_VERSION=\"v1.20.4+k3s1\" \\ INSTALL_K3S_EXEC=\"--tls-san {{.Values.node.hostname}}\" \\ INSTALL_K3S_SELINUX_WARN=\"true\" \\ sh - # Install fleet kubectl apply -f /etc/k3s/manifests/fleet-config.yaml # Install system-upgrade-controller kubectl apply -f https://raw.githubusercontent.com/rancher/system-upgrade-controller/v0.6.2/manifests/system-upgrade-controller.yaml","categories":"","description":"Running k3s and Fleet on a cOS vanilla raw image\n","excerpt":"Running k3s and Fleet on a cOS vanilla raw image\n","ref":"/docs/tutorials/k3s_and_fleet_on_vanilla_image_example/","tags":"","title":"K3s + Fleet"},{"body":"cOS + k3s + fleet = :heart: This repository is a sample of a custom cOS derivative which updates are managed by fleet.\nIn this tutorial we will:\n Build a custom OS image to deploy in our cluster Setup a cluster with cOS, k3s and fleet Upgrade the cluster to our custom OS image with fleet  1) Build the OS image # IMAGE=quay.io/costoolkit/test-images:fleet-sample # cd os # docker build -t $IMAGE . 2) Push the docker image # docker push $IMAGE 3) Prepare a cOS VM Download an ISO, or a qcow image from the Github artifacts of cOS. Or generate an iso of the image (check here for another example).\nIf deploying on AWS/openstack/Cloud, use the fleet-cloud-init.yaml file as userdata. If deploying on baremetal/VMs, place fleet-cloud-init.yaml in /oem after install (or run the installer with cos-installer --config https://raw.githubusercontent.com/rancher-sandbox/cos-fleet-upgrades-sample/main/fleet-cloud-init.yaml $DEVICE).\nReboot, after some bootstraping time (check until all pods are running with watch kubectl get pods -A), you should have a k3s cluster with fleet and system-upgrade-controller deployed.\n4) Upgrade with fleet Add your fleet repository to the fleet cluster:\ncat \u003e example.yaml \u003c\u003c \"EOF\" apiVersion: fleet.cattle.io/v1alpha1 kind: GitRepo metadata: name: upgrade # This namespace is special and auto-wired to deploy to the local cluster namespace: fleet-local spec: # Everything from this repo will be ran in this cluster. You trust me right? repo: \"https://github.com/rancher-sandbox/cos-fleet-upgrades-sample\" branch: \"main\" paths: - manifests EOF kubectl apply -f example.yaml An example of how to trigger an upgrade with fleet is in manifests/upgrade.yaml. Edit the image with the one generated in the previous steps, and commit it to your fleet repository, At this point you should see the upgrade job to kick-in, the system will reboot afterwards.\n","categories":"","description":"Using fleet to trigger upgradeson cOS based derivatives\n","excerpt":"Using fleet to trigger upgradeson cOS based derivatives\n","ref":"/docs/tutorials/trigger_upgrades_with_fleet/","tags":"","title":"Trigger upgrades with K3s and Fleet"},{"body":"You can find the examples below in the examples folder.\nFrom standard images Besides using the cos-toolkit toolchain, it‚Äôs possible to create standard container images which are consumable by the vanilla cOS images (ISO, Cloud Images, etc.) during the upgrade and deploy phase.\nAn example of a Dockerfile image can be:\nARG LUET_VERSION=0.16.7 FROM quay.io/luet/base:$LUET_VERSION AS luet FROM opensuse/leap:15.3 ARG ARCH=amd64 ENV ARCH=${ARCH} RUN zypper in -y \\ ... # Copy the luet config file pointing to the upgrade repository COPY conf/luet.yaml /etc/luet/luet.yaml # Copy luet from the official images COPY --from=luet /usr/bin/luet /usr/bin/luet RUN luet install -y \\ toolchain/yip \\ utils/installer \\ system/cos-setup \\ system/immutable-rootfs \\ system/grub-config \\ system/cloud-config \\ utils/k9s \\ utils/nerdctl COPY files/ / RUN mkinitrd The important piece is that an image needs to ship at least toolchain/yip ,utils/installer and system/cos-setup. system/grub-config is a default grub configuration, while you could likewise supply your own with in /etc/cos/bootargs.cfg.\nYou can also generate an image directly from the ones that the CI is publishing, or also from scratch. See the full example in examples/standard.\nGenerating from CI image You can just use the published final images:\n# Pick one version from https://quay.io/repository/costoolkit/releases-opensuse?tab=tags FROM quay.io/costoolkit/releases-opensuse:cos-system-0.5.3-5 COPY files/ / ... See the full example in examples/cos-official.\nFrom scratch The luet image quay.io/luet/base contains just luet, and can be used to boostrap the base system from scratch:\nconf/luet.yaml:\nlogging:color:falseenable_emoji:falsegeneral:debug:falsespinner_charset:9repositories:- name:\"cos\"description:\"cOS official\"type:\"docker\"enable:truecached:truepriority:1verify:falseurls:- \"quay.io/costoolkit/releases-opensuse\"Dockerfile:\nFROM quay.io/luet/base:latest # Copy the luet config file pointing to the cOS repository ADD conf/luet.yaml /etc/luet/luet.yaml ENV USER=root SHELL [\"/usr/bin/luet\", \"install\", \"-y\", \"-d\"] RUN system/cos-container SHELL [\"/bin/sh\", \"-c\"] RUN rm -rf /var/cache/luet/packages/ /var/cache/luet/repos/ ENV TMPDIR=/tmp ENTRYPOINT [\"/bin/sh\"] See the full example in examples/scratch.\nCustomizations All the method above imply that the image generated will be the booting one, there are however several configuration entrypoint that you should keep in mind while building the image:\n Everything under /system/oem will be loaded during the various stage (boot, network, initramfs). You can check here for the cOS defaults. See 00_rootfs.yaml to customize the booting layout. /etc/cos/bootargs.cfg contains the booting options required to boot the image with GRUB /etc/cos-upgrade-image contains the default upgrade configuration for recovery and the booting system image  ","categories":"","description":"This document describes the requirements to create standard container images that can be used for `cOS` deployments\n","excerpt":"This document describes the requirements to create standard container ‚Ä¶","ref":"/docs/examples/creating_bootable_images/","tags":"","title":"Creating bootable images"},{"body":"cos-toolkit is a manifest to share a common abstract layer between derivatives inheriting the same featureset.\ncos is a Luet tree and derivatives can be Luet trees as well that inherit part of the compilation specs from cos.\nThose trees are then post-processed and converted to Dockerfiles when building packages, that in turn are used to build docker images and final artefacts.\nHigh level workflow The building workflow can be resumed in the following steps:\n Build packages from container images. This step generates build metadata (luet build / docker build / buildah ..) Add repository metadata and create a repository from the build phase (luet create-repo) (otherwise, optionally) publish the repository and the artefacts along (luet create-repo --push-images)  While on the client side, the upgrade workflow is:\n luet install (when upgrading from release channels) latest cos on a pristine image file or luet util unpack (when upgrading from specific docker images)  Note: The manual build steps are not stable and will likely change until we build a single CLI to encompass the cos-toolkit components, rather use source .envrc \u0026\u0026 cos-build for the moment being while iterating locally.\nExample The sample repository has the following layout:\n‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ .envrc ‚îú‚îÄ‚îÄ .github ‚îÇ¬†‚îî‚îÄ‚îÄ workflows ‚îÇ¬†‚îú‚îÄ‚îÄ build.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ test.yaml ‚îú‚îÄ‚îÄ .gitignore ‚îú‚îÄ‚îÄ iso.yaml ‚îú‚îÄ‚îÄ LICENSE ‚îú‚îÄ‚îÄ .luet.yaml ‚îú‚îÄ‚îÄ Makefile ‚îú‚îÄ‚îÄ packages ‚îÇ¬†‚îú‚îÄ‚îÄ sampleOS ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 02_upgrades.yaml ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 03_branding.yaml ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 04_accounting.yaml ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ build.yaml ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ definition.yaml ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ setup.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ sampleOSService ‚îÇ¬†‚îú‚îÄ‚îÄ 10_sampleOSService.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ build.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ definition.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ main.go ‚îî‚îÄ‚îÄ README.md In the detail:\n the packages directory is the sample Luet tree that contains the package definitions [1] which composes the derivative. For an overview of the package syntax and build process, see the official luet documentation .luet.yaml contains a configuration file for luet pointing to the cos repositories, used to fetch packages required in order to build the iso [2] and to fetch definitions from [3]. Makefile and .envrc are just wrappers around luet build and luet create-repo iso.yaml a YAML file that describes what packages to embed in the final ISO  Note: There is nothing special in the layout, and neither the packages folder naming is special. By convention we have chosen to put the compilation specs in the packages folder, the Makefile is just calling luet with a set of default parameters according to this setup.\nThe .envrc is provided as an example to automatize the build process: it will build a docker image with the required dependencies, check the development docs about the local requirements if you plan to build outside of docker.\n[1] In the sample above we just declare two packages: sampleOS and sampleOSService. Their metadata are respectively in packages/sampleOS/definition.yaml and packages/sampleOSService/definition.yaml\n[2] We consume live/systemd-boot and live/syslinux from cos instead of building them from the sample repository\n[3] see also using git submodules instead\nSingle image OS Derivatives are composed by a combination of specs to form a final package that is consumed as a single image OS.\nThe container image during installation and upgrade, is converted to an image file with a backing ext2 fs.\nIn the sample repository we have defined system/sampleOS as our package, that will later on will be converted to image.\nPackages in luet have runtime and buildtime specifications into definition.yaml and build.yaml respectively, and in the buildtime we set:\nrequires:- category:\"system\"name:\"cos\"version:\"\u003e=0\"- category:\"app\"name:\"sampleOSService\"version:\"\u003e=0\"This instruct luet to compose a new image from the results of the compilation of the specified packages, without any version constraints, and use it to run any steps and prelude on top of it.\nWe are interested in the dependencies final images, and not the containers used to build them, so we enable requires_final_images:\nrequires_final_images: true We later run arbitrary steps to tweak the image:\nsteps:- ...And we instruct luet to compose the final artifact as a squash of the resulting container image, composed of all the files:\nunpack:trueA detailed explaination of all the keywords available is in the luet docs along with the supported build strategies.\nWe exclude then a bunch of file that we don‚Äôt want to be in the final package (regexp supported):\nexcludes:- ..Note: In the EpinioOS sample, we use requires instead of join:\nrequires:- category:\"system\"name:\"cos\"version:\"\u003e=0\"- name:\"k3s\"category:\"app\"version:\"\u003e=0\"- name:\"policy\"category:\"selinux\"version:\"\u003e=0\"The difference is that with requires we use the building container that was used to build the packages instead of creating a new image from their results: we are not consuming their artifacts in this case, but the environment used to build them. See also the luet docs for more details.\nBuilding Refering to the sampleOS example, we set the Makefile accordingly to compile the system package.\nWith luet installed locally and docker running, in your git checkout you can build it also by running luet build --tree packages system/sampleOS. This will produce an artifact of system/sampleOS. Similary, we could also build separately the sample application with luet build --tree packages app/sampleOSService.\nThe build process by default results in a build folder containing the package and the compilation metadata in order to generate a repository.\nNote on reproducibility: See the difference between our two samples repositories for an explanation of what are the implications of using a .luet.yaml file for building instead of a git submodule.\nAdditional packages In our sample repo we have split the logic of a separate application in app/sampleOSService.\nsampleOSService is just an HTTP server that we would like to have permanently in the system and on boot.\nThus we define it as a dependency in the system/sampleOS‚Äôs requires section:\nrequires:...- category:\"app\"name:\"sampleOSService\"version:\"\u003e=0\"Note If you are wondering about copying just single files, there is an upstream open issue about it.\nIn this way, when building our sampleOS package, luet will automatically apply the compilation spec of our package on top.\nTemplating The package build definition supports templating, and global interpolation of build files with multiple values files.\nValues file can be specified during build time in luet with the --values flag (also multiple files are allowed) and, if you are familiar with helm it using the same engine under the hood, so all the functions are available as well.\ncos-toolkit itself uses default values files for every supported distributions.\nFor a more complex example involving values file, see the epinio appliance example.\nTemplates uses cases are for: resharing common pieces between flavors, building for different platforms and architectures, ‚Ä¶\nUpgrades In order for the derivative to upgrade, it needs to be configured in order to download upgrades from a source.\nBy default, cos derivatives if not specified will point to latest cos-toolkit. To override, you need to or overwrite the content of /system/oem/02_upgrades.yaml or supply an additional one, e.g. /system/oem/03_upgrades.yaml in the final image, see an example here.\nThe configuration need to point to a specific docker image or an upgrade channel, a complete example and documentation is here.\nSee also the dedicated section in the docs.\nOEM Customizations There are several way to customize a cos-toolkit derivative:\n declaratively in runtime with cloud-config file (by overriding, or extending) stateful, via build definition when running luet build.  For runtime persistence configuration, the only supported way is with cloud-config files, see the relevant docs.\nA derivative automatically loads and executes cloud-config files which are hooking into system stages.\nIn this way the cloud-config mechanism works also as an emitter event pattern - running services or programs can emit new custom stages in runtime by running cos-setup stage_name.\nFor an extensive list of the default OEM files that can be reused or replaced see here.\nSee also the dedicated section in the docs.\nSeparate image recovery A separate image recovery can be used during upgrades.\nTo set a default recovery image or a package, set RECOVERY_IMAGE into /etc/cos-upgrade-image. It allows to override the default image/package used during upgrades, see also upgrades.\nTo make an ISO with a separate recovery image as squashfs, you can either use the default from cOS, by adding it in the iso yaml file:\npackages:rootfs:..uefi:..isoimage:...- recovery/cos-imgThe installer will detect the squashfs file in the iso, and will use it when installing the system. You can customize the recovery image as well by providing your own: see the recovery/cos-img package definition as a reference.\nBuilding ISOs, Vagrant Boxes, OVA In order to build an iso at the moment of writing, we first rely on luet-makeiso. It accepts a YAML file denoting the packages to bundle in an ISO and a list of luet repositories where to download the packages from.\nA sample can be found here.\nTo build an iso from a local repository (the build process, automatically produces a repository in build in the local checkout):\nluet-makeiso ./iso.yaml --local build Where iso.yaml is the iso specification file, and --local build is an optional argument to use also the local repository in the build process.\nWe are then free to refer to packages in the tree in the iso.yaml file.\nFor Vagrant Boxes, OVA and QEMU images at the moment of writing we are relying on packer templates.\nKnown issues When building cOS or a cOS derivative, you could face different issues, this section provides a description of the most known ones, and way to workaround them.\nBuilding SELinux fails cOS by default has SELinux enabled in permissive mode. If you are building parts of cOS or cOS itself from scratch, you might encounter issues while building the SELinux module, like so:\nStep 12/13 : RUN checkmodule -M -m -o cOS.mod cOS.te \u0026\u0026 semodule_package -o cOS.pp -m cOS.mod ---\u003e Using cache ---\u003e 1be520969ead Step 13/13 : RUN semodule -i cOS.pp ---\u003e Running in c5bfa5ae92e2 libsemanage.semanage_commit_sandbox: Error while renaming /var/lib/selinux/targeted/active to /var/lib/selinux/targeted/previous. (Invalid cross-device link). semodule: Failed! The command '/bin/sh -c semodule -i cOS.pp' returned a non-zero code: 1 Error: while resolving join images: failed building join image: Failed compiling system/selinux-policies-0.0.6+3: failed building package image: Could not push image: raccos/sampleos:ffc8618ecbfbffc11cc3bca301cc49867eb7dccb623f951dd92caa10ced29b68 selinux-policies-system-0.0.6+3.dockerfile: Could not build image: raccos/sampleos:ffc8618ecbfbffc11cc3bca301cc49867eb7dccb623f951dd92caa10ced29b68 selinux-policies-system-0.0.6+3.dockerfile: Failed running command: : exit status 1 Bailing out make: *** [Makefile:45: build] Error 1 The issue is possibly caused by https://github.com/docker/for-linux/issues/480 . A workaround is to switch the storage driver of Docker. Check if your storage driver is overlay2, and switch it to devicemapper\nMulti-stage copy build fails While processing images with several stage copy, you could face the following:\n üêã Building image raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d done üì¶ 8/8 system/cos-0.5.3+1 ‚§ë üî® build system/selinux-policies-0.0.6+3 ‚úÖ Done üöÄ All dependencies are satisfied, building package requested by the user system/cos-0.5.3+1 üì¶ system/cos-0.5.3+1 Using image: raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d üì¶ system/cos-0.5.3+1 üêã Generating 'builder' image from raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d as raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed with prelude steps üöß warning Failed to download 'raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed'. Will keep going and build the image unless you use --fatal üöß warning Failed pulling image: Error response from daemon: manifest for raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed not found: manifest unknown: manifest unknown : exit status 1 üêã Building image raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed Sending build context to Docker daemon 9.728kB Step 1/10 : FROM raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d ---\u003e f1122e79b17e Step 2/10 : COPY . /luetbuild ---\u003e 4ff3e202951b Step 3/10 : WORKDIR /luetbuild ---\u003e Running in 7ec571b96c6f Removing intermediate container 7ec571b96c6f ---\u003e 9e05366f830a Step 4/10 : ENV PACKAGE_NAME=cos ---\u003e Running in 30297dbd21a3 Removing intermediate container 30297dbd21a3 ---\u003e 4c4838b629f4 Step 5/10 : ENV PACKAGE_VERSION=0.5.3+1 ---\u003e Running in 36361b617252 Removing intermediate container 36361b617252 ---\u003e 6ac0d3a2ff9a Step 6/10 : ENV PACKAGE_CATEGORY=system ---\u003e Running in f20c2cf3cf34 Removing intermediate container f20c2cf3cf34 ---\u003e a902ff95d273 Step 7/10 : COPY --from=quay.io/costoolkit/build-cache:f3a333095d9915dc17d7f0f5629a638a7571a01dcf84886b48c7b2e5289a668a /usr/bin/yip /usr/bin/yip ---\u003e 42fa00d9c990 Step 8/10 : COPY --from=quay.io/costoolkit/build-cache:e3bbe48c6d57b93599e592c5540ee4ca7916158461773916ce71ef72f30abdd1 /usr/bin/luet /usr/bin/luet e3bbe48c6d57b93599e592c5540ee4ca7916158461773916ce71ef72f30abdd1: Pulling from costoolkit/build-cache 3599716b36e7: Already exists 24a39c0e5d06: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists 378615c429f5: Already exists c28da22d3dfd: Already exists ddb4dd5c81b0: Already exists 92db41c0c9ab: Already exists 4f4fb700ef54: Already exists 6e0ca71a6514: Already exists 47debb886c7d: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists d0c9d0f8ddb6: Already exists e5a48f1f72ad: Pulling fs layer 4f4fb700ef54: Pulling fs layer 7d603b2e4a37: Pulling fs layer 64c4d787e344: Pulling fs layer f8835d2e60d1: Pulling fs layer 64c4d787e344: Waiting f8835d2e60d1: Waiting e5a48f1f72ad: Download complete e5a48f1f72ad: Pull complete 4f4fb700ef54: Verifying Checksum 4f4fb700ef54: Download complete 4f4fb700ef54: Pull complete 7d603b2e4a37: Verifying Checksum 7d603b2e4a37: Download complete 64c4d787e344: Verifying Checksum 64c4d787e344: Download complete 7d603b2e4a37: Pull complete 64c4d787e344: Pull complete f8835d2e60d1: Verifying Checksum f8835d2e60d1: Download complete f8835d2e60d1: Pull complete Digest: sha256:9b58bed47ff53f2d6cc517a21449cae686db387d171099a4a3145c8a47e6a1e0 Status: Downloaded newer image for quay.io/costoolkit/build-cache:e3bbe48c6d57b93599e592c5540ee4ca7916158461773916ce71ef72f30abdd1 failed to export image: failed to create image: failed to get layer sha256:118537d8997a08750ab1ac3d8e8575e40fe60e8337e02633b0d8a1287117fe78: layer does not exist Error: while resolving join images: failed building join image: failed building package image: Could not push image: raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d cos-system-0.5.3+1-builder.dockerfile: Could not build image: raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d cos-system-0.5.3+1-builder.dockerfile: Failed running command: : exit status 1 Bailing out make: *** [Makefile:45: build] Error 1 There is a issue open upstream about it. A workaround is to enable Docker buildkit with DOCKER_BUILDKIT=1.\n","categories":"","description":"This document summarize references to create derivatives with `cos-toolkit` by using the `luet` toolchain.\n","excerpt":"This document summarize references to create derivatives with ‚Ä¶","ref":"/docs/creating-derivatives/creating_derivatives/","tags":"","title":"Creating derivatives"},{"body":"By default, cos and derivatives will inherit an immutable setup. A running system will look like as follows:\n/usr/local - persistent (COS_PERSISTENT) /oem - persistent (COS_OEM) /etc - ephemeral /usr - read only / immutable This means that any changes that are not specified as cloud-init configuration are not persisting across reboots.\nYou can place persisting cloud-init files either in /oem or /usr/local/oem, cOS already supports cloud-init datasources, so you can use also load cloud-init configuration as standard userdata, depending on the platform. For more details on the cloud-init syntax, see the cloud-init configuration reference.\n","categories":"","description":"This section contains various articles relative on how to customize cOS, branding and behavior.\n","excerpt":"This section contains various articles relative on how to customize ‚Ä¶","ref":"/docs/customizing/","tags":"","title":"Customizing"},{"body":"By default you can login with the user root and cos.\nYou can change this by overriding /system/oem/04_accounting.yaml in the container image, or in the running system in the persistency folder.\nExamples  Changing root password Example accounting file  ","categories":"","description":"Default login, and how to override it\n","excerpt":"Default login, and how to override it\n","ref":"/docs/customizing/login/","tags":"","title":"Login"},{"body":"Derivatives that wish to override default configurations can do that by placing extra cloud-init file, or overriding completely /system/oem in the target image.\nThis is to setup for example, the default root password or the prefered upgrade channel.\n/system/oem/00_rootfs.yaml - defines the rootfs mountpoint layout setting /system/oem/01_defaults.yaml - systemd defaults (keyboard layout, timezone) /system/oem/02_upgrades.yaml - Settings for channel upgrades /system/oem/03_branding.yaml - Branding setting, Derivative name, /etc/issue content /system/oem/04_accounting.yaml - Default user/pass /system/oem/05_network.yaml - Default network setup /system/oem/06_recovery.yaml - Executes additional commands when booting in recovery mode If you are building a cOS derivative, and plan to release upgrades, you must override (or create a new file under /system/oem) the /system/oem/02_upgrades.yaml pointing to the docker registry used to deliver upgrades.\nSee also the example appliance\n","categories":"","description":"OEM configuration reserved to cOS and derivatives\n","excerpt":"OEM configuration reserved to cOS and derivatives\n","ref":"/docs/customizing/oem_configuration/","tags":"","title":"OEM configuration"},{"body":"cOS vanilla images by default are picking upgrades by the standard upgrade channel. It means it will always get the latest published cOS version by our CI.\nHowever, it‚Äôs possible to tweak the default behavior of cos-upgrade to point to a specific docker image/tag, or a different release channel.\n/etc/cos-upgrade-image This file is read from cos-upgrade during start and allows to tweak the following:\n# Tweak the package to upgrade to, or the docker image (full reference) UPGRADE_IMAGE=system/cos # Turn on/off channel upgrades. If disabled, UPGRADE_IMAGE should be a full reference to a container image CHANNEL_UPGRADES=true # Turn on/off image signature checking. This is enabled by default when receiving upgrades from official channel VERIFY=false # Specify a separate recovery image (defaults to UPGRADE_IMAGE) RECOVERY_IMAGE=recovery/cos An example on how to tweak this file via cloud-init is available here\nChanging the default release channel To change the default release channel, set a /etc/luet/luet.yaml configuration file pointing to a valid luet repository:\n# For a full reference, see:# https://luet-lab.github.io/docs/docs/getting-started/#configurationlogging:color:falseenable_emoji:falsegeneral:debug:falsespinner_charset:9repositories:- name:\"sampleos\"description:\"sampleOS\"type:\"docker\"enable:truecached:truepriority:1verify:falseurls:- \"quay.io/costoolkit/releases-opensuse\"An example on how to tweak this file via cloud-init is available here\n","categories":"","description":"Customizing the default upgrade channel\n","excerpt":"Customizing the default upgrade channel\n","ref":"/docs/customizing/upgrades/","tags":"","title":"Upgrades"},{"body":"Requirements:\n Packer AWS access keys with the appropriate roles and permissions A Vanilla AMI Packer templates  The suggested approach is based on using Packer templates to customize the deployment and automate the upload and publish to AWS of cOS derivatives or cOS itself. For all the details and possibilties of Packer check the official documentation.\nRun the build with Packer Publishing an AMI image in AWS based on top of the latest cOS Vanilla image is fairly simple. In fact, it is only needed to set the AWS credentials and run a packer build process to trigger the deployment and register the resulting snapshot as an AMI. In such case the lastest cOS image will be deployed and configured with pure defaults. Consider:\n# From the root of a cOS-toolkit repository checkout \u003e export AWS_ACCESS_KEY_ID=\u003cyour_aws_access_key\u003e \u003e export AWS_SECRET_ACCESS_KEY=\u003cyour_aws_secret_access_key\u003e \u003e export AWS_DEFAULT_REGION=\u003cyour_aws_default_region\u003e \u003e cd packer \u003e packer build -only amazon-ebs.cos . AWS keys can be passed as environment variables as it is above or packer picks them from aws-cli configuration files (~/.aws) if any. Alternatively, one can define them in the variables file.\nThe -only amazon-ebs.cos flag is just to tell packer which of the sources to make use for the build. Note the packer/images.json.pkr.hcl file defines few other sources such as qemu and virtualbox.\nCustomize the build with a variables file The packer template can be customized with the variables defined in packer/variables.pkr.hcl. These are the variables that can be set on run time using the -var key=value or -var-file=path flags. The variable file can be a json file including desired varibles. Consider the following example:\n# From the packer folder of the cOS-toolkit repository checkout \u003e cat \u003c\u003c EOF \u003e test.json { \"aws_cos_install_args\": \"cos-deploy\", \"aws_launch_volume_size\": 16, \"name\": \"MyTest\" } EOF \u003e packer build -only amazon-ebs.cos -var-file=test.json . The above example runs the AMI Vanilla image on a 16GiB disk and calls the command cos-deploy to deploy the main OS, once deployed an snapshot is created and an AMI out this snapshot is registered in EC2. The created AMI artifact will be called MyTest, the name has no impact in the underlaying OS.\nAvailable variables for customization All the customizable variables are listed in packer/variables.pkr.hcl, variables with the aws_ prefix are the ones related to the AWS Packer template. These are some of the relevant ones:\n  aws_cos_install_args: This the command that will be executed once the Vanilla image booted. In this stage it is expected that user sets a command to install the desired cOS or derivative image. By default it is set to cos-deploy which will deploy the latest cOS image in cOS repositories. To deploy custom derivatives something like cos-deploy --docker-image \u003cmy-derivative-img-ref\u003e should be sufficient.\n  aws_launch_volume_size: This sets the disk size of the VM that Packer launches for the build. During Vanilla image first boot the system will expand to the disk geometry. The layout is configurable with the user-data.\n  aws_user_data_file: This sets the user-data file that will be used for the aws instance during the build process. It defaults to aws/setup-disk.yaml and the defauklt file basically includes the disk expansion configuration. It adds a COS_STATE partition that should be big enough to store about three times the size of the image to deploy. Then it also creates a COS_PERSISTENT partition with all the rest of the available space in disk.\n  aws_source_ami_filter_name: This a filter to choose the AMI image for the build process. It defaults to *cOS*Vanilla* pattern to pick the latest cOS Vanilla image available.\n  ","categories":"","description":"This section documents the procedure to deploy cOS (or derivatives) images in AWS public cloud provider by using the cOS Vanilla image.\n","excerpt":"This section documents the procedure to deploy cOS (or derivatives) ‚Ä¶","ref":"/docs/creating-derivatives/build_ami/","tags":"","title":"Build AMI machines for AWS"},{"body":"Requirements:\n cOS-toolkit source Google Cloud access keys with the appropriate roles and permissions gsutil and gcloud tools  The suggested approach high level view is building cOS packages and generating a RAW image from them. That would allow us to transform that RAW image in a valid Google Cloud blob that can be transformed into a VM image ready to be launched.\nThis generates a vanilla image that boots into recovery mode and can be used to deploy whatever image you want to the VM. Then you can snapshot that VM into a VM image ready to deploy with the default cOS system or your derivative.\nBuilding the packages We need to have the packages built locally in order to generate a proper RAW image. Just run:\nsudo make build All the artifacts will be generated under the build directory.\nSee also the dedicated section in the docs for more information about building cOS.\nBuilding the RAW image The RAW image is just a RAW disk image that contains the recovery, so once launched is ready to be used for installing whatever cOS or derivative that you want into the VM disks. This allows us to have a barebones base image that can be used for provisioning whatever cOS you want.\nBuilding the RAW image is as simple as running:\nsudo make raw_disk This will output a disk.raw fiel in the current directory which we will modify to make it work with Google Cloud.\nHINT: You can use this disk.raw locally as a disk for qemu if you want to check that it boots properly.\nTransform the RAW image into a compatible Google Cloud blob Currently importing images from storage blobs on Google Cloud have a few requirements:\n blobs have to be tar.gzipped with the flag --format=oldgnu the disk.raw has to be rounded up to the next Gb ( so no 2.1gb images or 2.4, they need to be an exact 3Gb or 2Gb) image inside the tar.gzip blob needs to be called disk.raw  We provide a make target that will do this for you:\nsudo make gce_disk This will create a disk.raw.tar.gz which is our final artifact\nUploading to Google Cloud storage and importing it as an image The last step is to upload the blob to Google Cloud storage and import that blob as a valid image.\nWith gsutil and gcloud tools installed and their credentials configured, you upload the blob with:\ngsutil cp disk.raw.tar.gz gs://YOURBUCKET/ Where YOURBUCKET is the destination bucket you want your file to end up in.\nAnd import it as an image with:\ngcloud compute images create IMAGENAME --source-uri=SOURCEURI --guest-os-features=UEFI_COMPATIBLE Where:\n IMAGENAME: The name for the final image SOURCEURI: The full Google Cloud Storage URI where the disk image is stored. This file must be a gzip-compressed tarball whose name ends in .tar.gz. This is the full path to the blob we just uploaded.  Note that we used --guest-os-features=UEFI_COMPATIBLE as a flag. This indicates that the image will be booted with UEFI and its required. Otherwise, launching the image will try to boot it in legacy mode and it will fail.\nOnce this is over you will have you cOS (or derivative) vanilla image ready for consumption. You can see your new image by running:\ngcloud compute images describe IMAGENAME ","categories":"","description":"This section documents the procedure to deploy cOS images to Google Cloud provider from scratch.\n","excerpt":"This section documents the procedure to deploy cOS images to Google ‚Ä¶","ref":"/docs/creating-derivatives/building_gce_images/","tags":"","title":"Build Google Cloud images"},{"body":"All the documentation below imply that the container image generated will be the booting one, there are however several configuration entrypoint that you should keep in mind while building the image which are general across all the implementation:\n Custom persistent runtime configuration has to be provided in /system/oem for derivatives, see also the documentation section. Everything under /system/oem will be loaded during the various stages (boot, network, initramfs). You can check here for the cOS defaults. See 00_rootfs.yaml to customize the booting layout. /etc/cos/bootargs.cfg contains the booting options required to boot the image with GRUB, see grub customization /etc/cos-upgrade-image contains the default upgrade configuration for recovery and the booting system image, see customizing upgrades  Derivatives inherits cOS defaults, which you can override during the build process, however there are some defaults which are relevant and listed below:\n","categories":"","description":"Documents various methods for creating cOS derivatives\n","excerpt":"Documents various methods for creating cOS derivatives\n","ref":"/docs/creating-derivatives/","tags":"","title":"Creating derivatives"},{"body":"COS is set to deploy a persistent grub.cfg into the COS_RECOVERY partition during the system installation or image creation. COS grub configuration includes three menu entries: first for the main OS system, second for the fallback OS system and a third for the recovery OS.\nFor example the main OS system menu entry could be something like:\nmenuentry \"cOS\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd initrd (loop0)$initramfs } Someting relevant to note is that the kernel parameters are not part of the persistent grub.cfg file stored in COS_RECOVERY partition. Kernel parameters are sourced from the loop device of the OS image to boot. This is mainly to keep kernel parameters consistent across different potential OS images or system upgrades.\nIn fact, cOS images and its derivatives, are expected to include a /etc/cos/bootargs.cfg file which provides the definition of the following variables:\n $kernel: Path of the kernel binary $kernelcmd: Kernel parameters $initramfs: Path of the initrd binary  This is the mechanism any cOS image or cOS derivative has to communicate its boot parameters (kernel, kernel params and initrd file) to GRUB2.\nFor example a cOS bootarg.cfg file could be:\nset kernel=/boot/vmlinuz if [ -n \"$recoverylabel\" ]; then # Boot arguments when the image is used as recovery set kernelcmd=\"console=tty1 root=live:CDLABEL=$recoverylabel rd.live.dir=/ rd.live.squashimg=$img panic=5\" else # Boot arguments when the image is used as active/passive set kernelcmd=\"console=tty1 root=LABEL=$label iso-scan/filename=$img panic=5 security=selinux selinux=1\" fi set initramfs=/boot/initrd You can tweak that file to suit your needs if you need to specify persistent boot arguments.\nGrub environment variables cOS (since v0.5.8) makes use of the GRUB2 environment block which can used to define persistent GRUB2 variables across reboots.\nThe default grub configuration loads the /grubenv of any available device and evaluates on next_entry variable and saved_entry variable. By default none is set.\nThe default boot entry is set to the value of saved_entry, in case the variable is not set grub just defaults to the first menu entry.\nnext_entry variable can be used to overwrite the default boot entry for a single boot. If next_entry variable is set this is only being used once, GRUB2 will unset it after reading it for the first time. This is helpful to define the menu entry to reboot to without having to make any permanent config change.\nUse grub2-editenv command line utility to define desired values.\nFor instance use the following command to reboot to recovery system only once:\n\u003e grub2-editenv /oem/grubenv set next_entry=recovery Or to set the default entry to fallback system:\n\u003e grub2-editenv /oem/grubenv set default=fallback These examples make of the COS_OEM device, however it could use any device detected by GRUB2 that includes the file /grubenv. First match wins.\n","categories":"","description":"GRUB 2 Configuration\n","excerpt":"GRUB 2 Configuration\n","ref":"/docs/customizing/configure_grub/","tags":"","title":"GRUB"},{"body":"The immutable rootfs concept in cOS is provided by a dracut module which is basically the contents of the immutable-rootfs luet package provided as part of the cOS repository tree.\nThe dracut module is mostly configured via kernel command line parameters or via the /run/cos/cos-layout.env environment file.\nThe immutable rootfs module is the responsible of mounting the root tree at boot time with the immutable specific setup. The immutability concept refers to read only root (/) system. To ensure the linux OS is still functional certain paths or areas are required to be writable, in those cases an ephemeral overaly tmpfs is set in place. Additionaly, the immutable rootfs module can also mount a custom list of device blocks with read write permissions, those are mostly devoted to store persistent data.\nThese are the read write paths the module mounts as part of the overlay ephemeral tmpfs: /etc, /root, /home, /opt, /srv, /usr/local and /var.\nThese paths will be all ephemeral unless there is a block device configured to be mounted in the same path.\nIt is important to remark all the immutable root configuration is applied in initrd before switching root and after rootfs cloud-init stage but before initramfs stage. So immutable rootfs configuration via cloud-init using the /run/cos/cos-layout.env file is only effective if called in any of the rootfs.before, rootfs or rootfs.after cloud-init stages.\nKernel configuraton paramters The immutable rootfs can be configured witht he following kernel parameters:\n  cos-img/filename=\u003cimgfile\u003e: This is one of the main parameters, it defines the location of the image file to boot from.\n  rd.cos.overlay=tmpfs:\u003csize\u003e: This defines the size of the tmpfs used for the ephemeral overlayfs. It can be expressed in MiB or as a % of the available memory. Defaults to rd.cos.overlay=tmpfs:20% if not present.\n  rd.cos.overlay=LABEL=\u003cvol_label\u003e: Optionally and mostly for debugging purposes the overlayfs can be mounted on top of a persistent block device. Block devices can be expressed by LABEL (LABEL=\u003cblk_label\u003e) or by UUID (UUID=\u003cblk_uuid\u003e)\n  rd.cos.mount=LABEL:\u003cblk_label\u003e:\u003cmountpoint\u003e: This option defines a persistent block device and its mountpoint. Block devices can also be defined by UUID (UUID=\u003cblk_uuid\u003e:\u003cmountpoint\u003e). This option can be passed multiple times.\n  rd.cos.oemtimeout=\u003cseconds\u003e: cOS by default assumes the existence of a persistent block device labelled COS_OEM which is used to keep some configuration data (mostly cloud-init files). The immutable rootfs tries to mount this device at very early stages of the boot even before applying the immutable rootfs configs. It done this way to enable to configure the immutable rootfs module within the cloud-init files. As the COS_OEM device might not be always present the boot process just continues without failing after a certain timeout. This option configures such a timeout. Defaults to 10s.\n  rd.cos.debugrw: This is a boolean option, true if present, false if not. This option sets the root image to be mounted as a writable device. Note this completely breaks the concept of an immutable root. This is helpful for debugging or testing purposes, so changes persist across reboots.\n  rd.cos.disable: This is a boolean option, true if present, false if not. It disables the execution of any immutable rootfs module logic at boot.\n  Configuration with an environment file The immutable rootfs can be configured with the /run/cos/cos-layout.env environment file. It is important to note that all the immutable root configuration is applied in initrd before switching root and after rootfs cloud-init stage but before initramfs stage. So immutable rootfs configuration via cloud-init using the /run/cos/cos-layout.env file is only effective if called in any of the rootfs.before, rootfs or rootfs.after cloud-init stages.\nIn the environment file only few options are available:\n  VOLUMES=LABEL=\u003cblk_label\u003e:\u003cmountpoint\u003e: This variable expects a block device and it mountpoint pair space separated list. The default cOS configuration is:\nVOLUMES=\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"\n  OVERLAY=: It defines the underlaying device for the overlayfs as in rd.cos.overlay= kernel parameter.\n  DEBUGRW=true: Sets the root (/) to be mounted with read/write permissions.\n  MERGE=true: Sets makes the VOLUMES values to be merged with any other volume that might have been defined in the kernel command line. The merging criteria is simple: any overlapping volume is overwritten all others are appended to whatever was already defined as a kernel parameter. If not defined defaults to true.\n  For exmaple a common cOS configuration can is expressed as part of the cloud-init configuration as follows:\nname:examplestage:rootfs:- name:\"Layout configuration\"environment_file:/run/cos/cos-layout.envenvironment:VOLUMES:\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"OVERLAY:\"tmpfs:25%\"","categories":"","description":"Immutable root filesystem configuration parameters\n","excerpt":"Immutable root filesystem configuration parameters\n","ref":"/docs/reference/immutable_rootfs/","tags":"","title":"Immutable Root Filesystem"},{"body":"To upgrade an installed system, just run cos-upgrade and reboot.\nHow it works cOS during installation sets two .img images files in the COS_STATE partition:\n /cOS/active.img labeled COS_ACTIVE: Where cOS typically boots from /cOS/passive.img labeled COS_PASSIVE: Where cOS boots for fallback  Those are used by the upgrade mechanism to prepare and install a pristine cOS each time an upgrade is attempted.\nUpgrade to a specific container image To specify a single container image to upgrade to instead of the regular upgrade channels, run cos-upgrade --docker-image image.\nNote by default cos-upgrade --docker-image checks images against the notary registry server for valid signatures for the images tag. To disable image verification, run cos-upgrade --no-verify --docker-image.\nSee the sample repository readme on how to tweak the upgrade channels for the derivative and a further description is available here\nFrom ISO The ISO can be also used as a recovery medium: type cos-upgrade from a LiveCD. It will then try to upgrade the image of the active partition installed in the system.\nIntegration with System Upgrade Controller If running a kubernetes cluster on the cOS system, you can leverage the system-upgrade-controller to trigger upgrades to specific image versions, for example:\n---apiVersion:upgrade.cattle.io/v1kind:Planmetadata:name:cos-upgradenamespace:system-upgradelabels:k3s-upgrade:serverspec:concurrency:1version:fleet-sample# Image tagnodeSelector:matchExpressions:- {key: k3s.io/hostname, operator:Exists}serviceAccountName:system-upgradecordon:true# drain:# force: trueupgrade:image:quay.io/costoolkit/test-images# Image upgrade referencecommand:- \"/usr/sbin/suc-upgrade\"See also trigger upgrades with fleet\n","categories":"","description":"How to run upgrades in cOS\n","excerpt":"How to run upgrades in cOS\n","ref":"/docs/getting-started/upgrading/","tags":"","title":"Upgrading"},{"body":"","categories":"","description":"Examples and recipes for using the vanilla cOS image\n","excerpt":"Examples and recipes for using the vanilla cOS image\n","ref":"/docs/examples/","tags":"","title":"Examples"},{"body":"cOS derivatives have a recovery mechanism built-in which can be leveraged to restore the system to a known point. At installation time, the recovery partition is created from the installation medium.\nRecovery partition A derivative can be recovered anytime by booting into the  recovery partition and by running cos-reset from it.\nThis command will regenerate the bootloader and the images in the COS_STATE partition by using the recovery image.\nUpgrading the recovery partition The recovery partition can also be upgraded by running\ncos-upgrade --recovery from either the active or passive partition.\nIt also supports to specify docker images directly:\ncos-upgrade --recovery --docker-image \u003cimage\u003e Note: the command has to be run in the standard partitions used for boot (Active or Fallback).\nUpgrading from the recovery partition The recovery partition can upgrade also the active system by running cos-upgrade, and it also supports to specify docker images directly:\ncos-upgrade --recovery --docker-image \u003cimage\u003e ","categories":"","description":"How to use the recovery partition to reset the system or perform upgrades.\n","excerpt":"How to use the recovery partition to reset the system or perform ‚Ä¶","ref":"/docs/getting-started/recovery/","tags":"","title":"Recovery"},{"body":"Installing required dependencies for local build To get requirements installed locally, run:\n$\u003e make deps or you need:\n luet luet-makeiso squashfs-tools  zypper in squashfs on SLES or openSUSE   xorriso  zypper in xorriso on SLES or openSUSE   yq (version 3.x), installed via packages/toolchain/yq (optional) jq, installed via packages/utils/jq (optional)  Note: Running make deps will install only luet, luet-makeiso, yq and jq. squashfs-tools and xorriso needs to be provided by the OS.\nManually install dependencies To install luet locally, you can also run as root:\n# curl https://raw.githubusercontent.com/rancher-sandbox/cOS-toolkit/master/scripts/get_luet.sh | sh or build luet from source).\nYou can find more luet components in the official Luet repository.\nluet-makeiso luet-makeiso comes with cOS-toolkit and can be installed with luet locally:\n$\u003e luet install -y toolchain/luet-makeiso You can also grab the binary from luet-makeiso releases.\nyq and jq yq (version 3.x) and jq are used to retrieve the list of packages to build in order to produce the final ISOs. Those are not strictly required, see the Note below.\nThey are installable with:\n$\u003e luet install -y utils/jq toolchain/yq Note: yq and jq are just used to generate the list of packages to build, and you don‚Äôt need to have them installed if you manually specify the packages to be compiled.\n","categories":"","description":"Building prerequisites\n","excerpt":"Building prerequisites\n","ref":"/docs/development/dependencies/","tags":"","title":"Build requirements"},{"body":"cOS toolkit High level Architecture This page tries to encompass the cos-toolkit structure and the high level architecture, along with all the involved components.\nDesign goals  Blueprints to build immutable Linux derivatives from container images A workflow to maintain, support and deliver custom-OS and upgrades to end systems Derivatives have the same ‚Äúfoundation‚Äù manifest - easy to customize on top, add packages: systemd, dracut and grub as a foundation stack. Upgrades delivered with container registry images ( also workflow with docker run \u0026\u0026 docker commit supported! ) The content of the container image is the system which is booted.  High level overview cOS-Toolkit encompasses several components required for building and distributing OS images. This issue summarize the current state, and how we plan to integrate them in a single CLI to improve the user experience.\ncOS-Toolkit is also a manifest, which includes package definitions of how the underlying OS is composed. It forms an abstraction layer, which is then translated to Dockerfiles and built by our CI (optionally) for re-usal. A derivative can be built by parts of the manifest, or reusing it entirely, container images included.\nThe fundamental phases can be summarized in the following steps:\n Build packages from container images (and optionally keep build caches) Extract artefacts from containers Add metadata(s) and create a repository (optionally) publish the repository and the artefacts  The developer of the derivative applies a customization layer during build, which is an augmentation layer in the same form of cos-toolkit itself. An example repository is provided that shows how to build a customOS that can be maintained with a container image registry.\nDistribution The OS delivery mechanism is done via container registries. The developer that wants to provide upgrades for the custom OS will push the resulting container images to the container registry. It will then be used by the installed system to pull upgrades from.\nUpgrade mechanism There are two different upgrade mechanisms available that can be used from a maintainer perspective: (a) release channels or (b) providing a container image reference ( e.g. my.registry.com/image:tag ) that can be tweaked in the customization phases to achieve the desired effect.\n","categories":"","description":"High level architecture of cOS and its components.\n","excerpt":"High level architecture of cOS and its components.\n","ref":"/docs/reference/high_level_architecture/","tags":"","title":"High level architecture"},{"body":"","categories":"","description":"cOS tutorials and real life use-case samples\n","excerpt":"cOS tutorials and real life use-case samples\n","ref":"/docs/tutorials/","tags":"","title":"Tutorials"},{"body":"Derivatives  Creating derivatives Creating bootable images Derivatives featureset  Samples  Sample repository EpinioOS sample repository Use Fleet to upgrade a cOS derivative Deploy Fleet on a cOS vanilla image  cOS development  Development notes High Level architecture Github project for a short-term Roadmap  Usage hints  Grub2 default boot entry setup  ","categories":"","description":"References for cOS derivatives, like common featuresets, high level architecture\n","excerpt":"References for cOS derivatives, like common featuresets, high level ‚Ä¶","ref":"/docs/reference/","tags":"","title":"Reference"},{"body":"Welcome!\nThe cOS (containerized OS) distribution is entirely built over GitHub. You can check the pipelines in the .github folder to see how the process looks like.\nRepository layout  packages: contain packages definition for luet values: interpolation files, needed only for multi-arch and flavor-specific build assets: static files needed by the iso generation process packer: Packer templates tests: cOS test suites manifest.yaml: Is the manifest needed used to generate the ISO and additional packages to build  Forking and test on your own By forking the cOS-toolkit repository, you already have the Github Action workflow configured to start building and pushing your own cOS fork.\nThe only changes required to keep in mind for pushing images:\n set DOCKER_PASSWORD and DOCKER_USERNAME as Github secrets, which are needed to push the resulting container images from the pipeline. Tweak or set the Makefile‚Äôs REPO_CACHE and FINAL_REPO accordingly. Those are used respectively for an image used for cache, and for the final image reference.  Those are not required for building - you can disable image push (--push) from the Makefile or just by specifying e.g. BUILD_ARGS=--pull when calling the make targets.\nBuilding locally cOS has a container image which can be used to build cOS locally in order to generate the cOS packages and the cOS iso from your checkout.\nFrom your git folder:\n$\u003e docker build -t cos-builder . $\u003e docker run --privileged=true --rm -v /var/run/docker.sock:/var/run/docker.sock -v $PWD:/cOS cos-builder or use the .envrc file:\n$\u003e source .envrc $\u003e cos-build Build all packages locally Building locally has a set of dependencies that should be satisfied.\nThen you can run\n# make build as root\nTo clean from previous runs, run make clean.\nNote: The makefile uses yq and jq to retrieve the packages to build from the iso specfile.\nIf you don‚Äôt have jq and yq installed, you must pass by the packages manually with PACKAGES (e.g. PACKAGES=\"system/cos live/systemd-boot live/boot live/syslinux\").\nYou might want to build packages running as root or sudo -E if you intend to preserve file permissions in the resulting packages (mainly for xattrs, and so on).\nBuild ISO If using SLES or openSUSE, first install the required deps:\n# zypper in -y squashfs xorriso dosfstools and then, simply run\n# make local-iso Testing ISO changes To test changes against a specific set of packages, you can for example:\n# make PACKAGES=\"toolchain/yq\" build local-iso root is required because we want to keep permissions on the output packages (not really required for experimenting).\nRun with qemu After you have the iso locally, run\n $\u003e QEMU=qemu-system-x86_64 make run-qemu This will create a disk image at .qemu/drive.img and boot from the ISO.\n If the image already exists, it will NOT be overwritten.\nYou need to run an explicit make clean_run to wipe the image and start over.\n Installing With a fresh drive.img, make run-qemu will boot from ISO. You can then log in as root with password cos and install cOS on the disk image with:\n# cos-installer /dev/sda Running After a successful installation of cOS on drive.img, you can boot the resulting sytem with\n $\u003e QEMU_ARGS=\"-boot c\" make run-qemu Run tests Requires: Virtualbox or libvirt, vagrant, packer\nWe have a test suite which runs over SSH.\nTo create the vagrant image:\n $\u003e PACKER_ARGS=\"-var='feature=vagrant' -only virtualbox-iso\" make packer To run the tests:\n $\u003e make test ","categories":"","description":"How to build cOS?\n","excerpt":"How to build cOS?\n","ref":"/docs/development/","tags":"","title":"Development"},{"body":"What is cOS? cOS is a toolkit which allows container images to be bootable in VMs, baremetals, embedded devices, and much more.\ncOS allows to create meta-Linux derivatives which are configured throuought cloud-init configuration files and are immutable by default.\ncOS and derivatives shares a common featureset, can be upgraded in a OTA-alike style, and upgrades are delivered with standard container registries.\ncOS comes also with vanilla images that can be used to boot directly container images built with the toolkit.\nWhy cOS? cOS allows to create custom OS versions in your cluster with standard container images with a high degree of customization. It can also be used in its vanilla form - cOS enables then everyone to build their own derivative and access it in various formats. It‚Äôs like ‚ÄúVentoy‚Äù for persistent systems.\nTo build a bootable image is as simple as running docker build.\n  What is it good for?: Embedded, Cloud, Containers, VM, Baremetals, Servers, IoT, Edge\n  What is it not good for?: Workstations (?), Gaming,\n  Design goals  A Manifest for container-based OS. It contains just the common bits to make a container image bootable and to be upgraded from, with few customization on top Immutable-first, but with a flexible layout Cloud-init driven Based on systemd Built and upgraded from containers - It is a single image OS! OTA updates Easy to customize Cryptographically verified instant switch from different versions recovery mechanism with cOS vanilla images (or bring your own)  ","categories":"","description":"","excerpt":"What is cOS? cOS is a toolkit which allows container images to be ‚Ä¶","ref":"/docs/","tags":"","title":"Documentation"},{"body":"  Immutable Linux Derivatives at your fingertips  cOS is a toolkit to build, ship and maintain cloud-init driven Linux derivatives with containers..  Learn More    --   Great For Embedded Appliances Edge True DevOps    Why Use cOS toolkit ?  Built for DevOps cOS toolkit allows to maintain custom Linux derivatives in a GitOps style with container registries.\n  Operational Happiness Upgrade machines from container images in OTA style - manually or automatically within kubernetes.\n  Bring your own OS A common featureset shared between cOS derivatives, completely pluggable.\n  Single Image OS cOS derivatives are single container image OS that are bootable from a system. They follow an OTA update style and gets upgrades from regular container registries.\n    Get Started Step 1.git clone https://github.com/rancher-sandbox/cos-toolkit Step 2.docker build -t example examples/standard Step 3.cos-upgrade example  That‚Äôs it! With just a few commands, you have your own custom OS.\n Learn More Ready to upgrade?\nBuild with cOS toolkit today     ","categories":"","description":"Immutable Linux Derivatives at your fingertips","excerpt":"Immutable Linux Derivatives at your fingertips","ref":"/","tags":"","title":"cOS toolkit"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]